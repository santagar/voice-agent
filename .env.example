OPENAI_API_KEY=your-openai-key

# Realtime server
REALTIME_PORT=4001
# Realtime models (voice + brain)
# Primary model used by the WS bridge (default: gpt-4o-mini-realtime-preview)
REALTIME_MODEL=gpt-4o-mini-realtime-preview
# Optional premium / higher-capacity variant you may switch to (not wired yet, for future routing logic)
REALTIME_MODEL_PREMIUM=gpt-4o-realtime-preview

# Realtime voice (for audio responses)
# Valid examples: alloy, ash, ballad, coral, echo, verse (etc.)
REALTIME_VOICE=verse

# Embeddings (KB / RAG)
# Used for knowledge ingestion, RAG queries, and scope detection
EMBEDDING_MODEL=text-embedding-3-small

# Server-side STT for user transcripts (/api/transcribe)
# Used only to render user bubbles and drive scope detection; Realtime still
# consumes audio directly via the realtime bridge.
TRANSCRIPTION_MODEL=gpt-4o-mini-transcribe

# Frontend audio tuning (lab)
# Assistant playback rate (1.0 = normal speed, 1.05 is a subtle bump)
NEXT_PUBLIC_ASSISTANT_PLAYBACK_RATE=1.05
# VAD: minimum RMS energy considered as voice (0.0â€“1.0)
NEXT_PUBLIC_VAD_VOICE_THRESHOLD=0.008
# VAD: silence duration (ms) to close a user utterance
NEXT_PUBLIC_VAD_SILENCE_MS=500

# Pinecone (optional, for ANN vector search)
PINECONE_API_KEY=
PINECONE_INDEX_HOST=
PINECONE_NAMESPACE=default
PINECONE_BATCH_SIZE=50
PINECONE_TOP_K=5

# External API for tool handlers (optional)
TOOL_API_BASE_URL=http://localhost:3000
CORE_API_TOKEN=
