OPENAI_API_KEY=your-openai-key

# Realtime server
REALTIME_PORT=4001
# Realtime models (voice + brain)
# Primary model used by the WS bridge (default: gpt-4o-mini-realtime-preview)
REALTIME_MODEL=gpt-4o-mini-realtime-preview
# Optional premium / higher-capacity variant you may switch to (not wired yet, for future routing logic)
REALTIME_MODEL_PREMIUM=gpt-4o-realtime-preview

# Realtime voice (for audio responses)
# Valid examples: alloy, ash, ballad, coral, echo, verse (etc.)
REALTIME_VOICE=verse

# Embeddings (KB / RAG)
# Used for knowledge ingestion, RAG queries, and scope detection
EMBEDDING_MODEL=text-embedding-3-small

# Server-side STT for user transcripts (/api/transcribe)
# Used only to render user bubbles and drive scope detection; Realtime still
# consumes audio directly via the realtime bridge.
TRANSCRIPTION_MODEL=gpt-4o-mini-transcribe

# Intent classification model (text-only) used to decide whether
# a user utterance captured while the assistant is speaking should
# be treated as a real turn (USER_TURN) or ignored as noise.
INTENT_MODEL=gpt-4o-mini

# Frontend audio tuning (lab)
# Assistant playback rate (1.0 = normal speed, 1.05 is a subtle bump)
NEXT_PUBLIC_ASSISTANT_PLAYBACK_RATE=1.05
# Client-side VAD (voice activity detection) – used only for UI
# (breathing dot, barge-in hints). Server-side VAD/denoise in the
# bridge is the primary filter for noise now.
NEXT_PUBLIC_VAD_VOICE_THRESHOLD=0.015
NEXT_PUBLIC_VAD_SILENCE_MS=600
NEXT_PUBLIC_VAD_NOISE_FACTOR=2.5

# Minimum ms of continuous detected user voice before we treat it as
# a true barge-in and interrupt the assistant's current response.
NEXT_PUBLIC_BARGE_IN_MIN_MS=220

# Voice transcription mode (frontend)
# When set to "false", the assistant runs in "pure voice" mode:
# - user voice utterances are not sent to /api/transcribe or /api/voice-intent
# - no user chat bubbles are created from voice
# - Realtime still consumes audio directly via the bridge and handles reasoning
NEXT_PUBLIC_USE_VOICE_TRANSCRIBE=true

# Server-side input VAD (bridge) using RNNoise – filters obvious background noise
# before forwarding audio to the Realtime API. Enabled by default.
INPUT_VAD_ENABLED=true
# Minimum number of VAD-positive frames required within a single chunk
# before it is treated as speech.
INPUT_VAD_MIN_FRAMES=2
# Optional: minimum fraction of frames within a chunk that must be
# classified as speech. Helps ignore tiny "mmm"/"eh" noises.
INPUT_VAD_MIN_SPEECH_FRACTION=0.2

# Pinecone (optional, for ANN vector search)
PINECONE_API_KEY=
PINECONE_INDEX_HOST=
PINECONE_NAMESPACE=default
PINECONE_BATCH_SIZE=50
PINECONE_TOP_K=5

# Prisma / database
# Default: local SQLite file under database/dev.db
DATABASE_URL="file:./database/dev.db"

# External API for tool handlers (optional)
TOOL_API_BASE_URL=http://localhost:3000
CORE_API_TOKEN=

# OAuth providers (optional)
GOOGLE_CLIENT_ID=
GOOGLE_CLIENT_SECRET=
